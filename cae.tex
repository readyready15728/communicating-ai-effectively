\documentclass[10pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[sfdefault]{FiraSans}
\usepackage{verbatim}
\usepackage{graphicx}
\usetheme{Antibes}
\usecolortheme{dolphin}

\title{Communicating AI Effectively}
\author{Lynn Bradshaw}
\date{10 August 2019}

\begin{document}
  \begin{frame}
    \maketitle
  \end{frame}

  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[pausesections,hideallsubsections]
  \end{frame}

  \section{Shameless Advertisement of the Presentation Medium}

  \subsection{The Advertisement}

  \begin{frame}
    \frametitle{How This Presentation Was Made}
    \begin{itemize}
      \item I can't say I'm not partial towards my tools.
      \pause
      \item This presentation was created in the typesetting system \LaTeX,
        using the Beamer presentation class.
      \pause
      \item Because \LaTeX{} is text, I was able to take advantage of Vim's
        advantages.
      \pause
      \item In \LaTeX, content is king.
      \pause
      \item There are however word processor-like frontends and other editors.
      \pause
      \item I'm a slob but I appreciate attention to detail in typography.
    \end{itemize}
  \end{frame}

  \subsection{Here's What It Looks Like}

  \begin{frame}
    \frametitle{The Header}
    \begin{center}
      \includegraphics[width=0.9\textwidth]{latex-header.png}
    \end{center}
  \end{frame}

  \begin{frame}
    \frametitle{Part of an Earlier Draft}
    \begin{center}
      \includegraphics[width=0.9\textwidth]{latex-section-and-frame.png}
    \end{center}
  \end{frame}

  \section{Don't Just Take My Word for It}

  \begin{frame}
    \frametitle{Don't Just Take My Word for It}

    \begin{itemize}
      \item This presentation will necessarily be opinionated.
      \pause
      \item Unfortunately I am fallible.
      \pause
      \item I may be right a lot of the time but you can't win them all.
      \pause
      \item (\ldots{}but pray that what I have to say about Skynet being
        unrealistic is true.)
      \pause
      \item Further research is heavily recommended.
      \pause
      \item At the end of the presentation, feel free to challenge or add to
        anything I've said here.
    \end{itemize}
  \end{frame}

  \section{Taxonomy of AI and Its Uses}

  \subsection{Why This Approach?}

  \begin{frame}
    \frametitle{Why This Approach?}

    \begin{itemize}
      \item Basically people more or less know what ``artificial
        intelligence'' is. The problem is that they don't know the details.
      \pause
      \item Compare biological taxonomy: knowing what animals, loosely
        speaking, are but not knowing the difference between the classes
        \textit{Annelida} (segmented worms) and \textit{Aves} (birds).
      \pause
      \item My taxonomy traces the historical development of the field, offering
        further understanding.
      \pause
      \item Having said this, there are different ways the field can be split
        up and none is decisively the best one.
      \pause
      \item (N.b.: to avoid making issues more complicated than they already
        are I will leave out robotics almost entirely.)
    \end{itemize}
  \end{frame}

  \subsection{Traditional Methods}

  % Check the Russell and Norvig text and historical material
  \begin{frame}
    \frametitle{Foundation of the Field}
    \begin{itemize}
      \item Occurred during the Dartmouth Summer Project of 1954.
      \pause
      \item Groundbreaking but preceded by decades and even centuries by
        precedents of notions about thinking or otherwise capable automata.
      \pause
      \item Core founders considered to be John McCarthy, Marvin Minsky,
        Herbert Simon and Allen Newell.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{The Results}
    \begin{itemize}
      \item The fraught name!
      \pause
      \item Departure from early experiments in neural networks (yes!) towards
        symbol processing.
      \pause
      \item The stunning early success of Logic Theorist.
      \pause
      \item The ball was now rolling.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Other Developments in the 50s and 60s}
    \begin{itemize}
      \item Early (finicky) character recognition.
      \pause
      \item Perceptrons.
      \pause
      \item Early statistical methods, including nearest neighbor.
      \pause
      \item Heuristic problem-solving methods, including General Problem
        Solver and some fairly decent checkers and chess programs.
      \pause
      \item (Very crappy) machine translation.
      \pause
      \item Semantic networks.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Some Very Ominous Quotes About AI from This Era}

    \begin{itemize}
      \item ``[If] one could devise a successful chess machine, one would seem
        to have penetrated to the core of human intellectual endeavor.'' ---
        Allen Newell, Cliff Shaw and Herbert Simon, 1958
      \item ``Machines will be capable, within twenty years, of doing any work
        a man can do.'' --- Herbert Simon, 1965
      \item ``Within a generation \ldots the problem of creating `artificial
        intelligence' will substantially be solved.'' --- Marvin Minsky, 1967
      \item ``In from three to eight years we will have a machine with the
        general intelligence of an average human being.'' --- Marvin Minsky,
        1970
      \pause
      \item ``Uh-oh\ldots'' --- Me, 2019
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{They Call Me Snow Miser\ldots}

    \includegraphics[width=0.9\textwidth]{snow-miser.jpg}
  \end{frame}

  \begin{frame}
    \frametitle{Not So Fast!}

    \begin{itemize}
      \item We're not quite in AI winter terrritory yet! There were some
        important achievements from the mid-60s to the 70s.
      \pause
      \item Early face recognition (Woody Bledsoe, Takeo Kanade et al.).
      \pause
      \item Three-dimensional object recognition (Lawrence G. Roberts, Seymour
        Papert et al.).
      \pause
      \item Image filtering methods still used today (e.g. the Sobel-Feldman
        operator).
      \pause
      \item A slow, crappy robot called Shakey.
      \pause
      \item SHRDLU.
      \pause
      \item DENDRAL.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{AI Winter(s, Plural)}

    \begin{itemize}
      \item From the 60s to the 80s several hype cycles resulted in AI winter
        events. These were caused by:
      \pause
      \item The ALPAC report of 1966.
      \pause
      \item The publication of \textit{Perceptrons} in 1969.
      \pause
      \item The Lighthill report of 1973.
      \pause
      \item US researchers somehow managing to exhaust (D)ARPA's patience.
      \pause
      \item The failed Fifth Generation Computer Systems initiative from
        Japan.
      \pause
      \item Excess optimism towards expert systems deflating.
      \pause
      \item Further cutbacks to DARPA initiatives.
      \pause
      \item The collapse of the Lisp machine market.
    \end{itemize}
  \end{frame}

  \subsection{Early Modern Methods}

  \begin{frame}
    \frametitle{A Quick Introduction}

    \begin{itemize}
      \item About the term ``early modern'': I stole it from world history.
        It's not official.
      \pause
      \item I'm going to be pretty brief in this subsection and the next
        because this is stuff you probably already mostly know about. I don't
        want to seem patronizing.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{80s Comeback}

    \begin{itemize}
      \item AI had quite a boom in the 80s!
      \pause
      \item Although a lot of the renewed interest in AI ultimately led to the
        failures and disappointments described previously, but some of the
        initiatives had staying power.
      \pause
      \item The most iconic achievement of this decade in the domain of AI was
        the popularization of connectionism. (Be sure to bring up Yann LeCun's
        research for the USPS.)
      \pause
      \item Semantic networks and the curious case of Doug Lenat.
      \pause
      \item Decision trees!
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{AI from 1993 to 2011: A Highlights Reel}

    \begin{itemize}
      \item More application of existing ideas than theoretical breakthroughs.
      \pause
      \item Euphemisms (``computational intelligence'' etc.) were used to
        ensure that applied work could receive funding. (Mention the Nick
        Bostrom quote.)
      \pause
      \item Hardware improvements finally make solid ideas feasible in
        practice, Deep Blue being a good example.
      \pause
      \item DARPA Grand Challenge victory (2005, 131 miles), DARPA Urban
        Challenge (2007, 55 miles).
      \pause
      \item In 2011, Watson stomps Brad Rutter and Ken Jennings in
        \textit{Jeopardy!}.
      \pause
      \item ``Where is HAL 9000?'' Various answers from Minsky, McCarthy,
        Kurzweil and Hawkins.
      \pause
      \item Bonus item: the Apriori association rule learning algorithm. The
        paper by Agrawal et al. has over 21,000 citations on Google Scholar!
    \end{itemize}
  \end{frame}

  % Prelude
  % Supervised, unsupervised
  % Emphasize relative simplicity of many algorithms

  \subsection{Deep Learning}

  \begin{frame}
    \frametitle{The Ore from Which the Metal Comes}

    \begin{itemize}
      \item I like to compare deep learning to the human brain but not in the
        naïve sense that many adhere to: what I mean is that the processing
        power can only justify itself if the data / metabolic requirements are
        met.
      \pause
      \item This is where ``big data'' comes in.
      \pause
      \item A famous paper from consulting firm McKinsey \& Company estimated
        that ``by 2009, nearly all sectors in the US economy had at least an
        average of 200 terabytes of stored data \ldots per company with at
        least 1,000 employees''.
      \pause
      \item The term ``big data'' has been in use since the 90s but its use
        really started taking off early in the 10s. It was not long thereafter
        that deep learning did. Coincidence?
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Surprisingly Early Foundations}

    \begin{itemize}
      \item The term ``deep learning'' was actually coined by computer
        scientist Rina Dechter in 1986 (although in a different context) and
        later used regarding neural networks by Igor Aizenberg et al. in a
        2000 paper (presumably in the current sense?)
      \pause
      \item However the earliest working learning algorithm for multilayer
        perceptrons was put forward by Oleksiy Ivakhnenko in \emph{1965}. (But
        remember that perceptrons are still linear.)
      \pause
      \item The Neocognitron of Kunihiko Fukushima, introduced in 1980, was a
        deep learning network used for computer vision and inspired current
        CNNs.
      \pause
      \item In 1989, Yann LeCun et al. used a standard backprop deep neural
        network to recognize handwritten ZIP codes on mail. It worked but
        training required three days. (Not that bad for the time?)
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Deep Learning Appears in Earnest}

    \begin{itemize}
      \item LSTM was discovered by Sepp Hochreiter and Jürgen Schmidhuber in
        1997 (!) and led to speech recognition competitive with traditional
        approaches in the early 00s, in some cases.
      \pause
      \item At around the same time, CNNs were already processing 10-20\% of
        checks written in the United States.
      \pause
      \item In 2006, Geoff Hinton et al. demonstrated efficient training of
        deep feedforward neural networks.
      \pause
      \item In 2009, Google and others began to use Nvidia GPUs to speed up
        DNN training by two orders of magnitude.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Deep Learning Revolution}

    \begin{itemize}
      \item In 2011, max-pooling CNNs trained on GPUs first achieved
        superhuman performance in a visual pattern recognition contests.
      \pause
      \item In the same year, this same approach won the ICDAR Chinese
        handwriting contest, later going on to win the ISBI image segmentation
        contest in 2012.
      \pause
      \item Deep learning's presence was also felt in the academic world, with
        a paper on the aforementioned approach by Ciresan et al. presented at
        the leading conference CVPR demonstrating dramatic improvements on
        vision benchmarks.
      \pause
      \item What have we seen since then? Fake Obama, color restoration, image
        description, OCR translation, winning Super Mario Bros., style
        transfer etc. etc. etc.
      \pause
      \item And you pretty much know the rest\ldots
    \end{itemize}
  \end{frame}

  \section{Assessing Realisticness of Applications}

  \subsection{Overview}

  \begin{frame}
    \frametitle{Overview}

    \begin{itemize}
      \item Broadly speaking, the current tendency in our society is to
        overestimate what is possible with the state of the art in AI rather
        than to underestimate it.
      \pause
      \item At the same time, we don't want to go too far. (Bring up the
        example of a Babelfish device with near-native proficiency.
        Attainable maybe?)
      \pause
      \item I like to use lots of analogies and stories instead of getting
        bogged down in technical details when I explain these sorts of things
        and I think I've included a few of my best here. You be the judge!
    \end{itemize}
  \end{frame}

  \subsection{The Current Hurdles to Perfection}

  \begin{frame}
    \frametitle{The Big One: Domain Specificity}

    \begin{itemize}
      \item No matter how good our current AIs are, they are \emph{one-trick
        ponies}!
      \pause
      \item Even the algorithms underlying something like AlphaGo, though
        increasingly general, have not made much progress towards AGI.
      \pause
      \item It's an easy mistake to make!
      \pause
      \item Give them the analogy of the Summer Olympics.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Embodiment (or the Lack Thereof)}

    \begin{itemize}
      \item Give them the analogy of the steam locomotive.
      \pause
      \item Embodiment and interaction with the physical world is the most
        compelling and possibly the only way to achieve truly autonomous AI.
      \pause
      \item Give them Rodney Brooks' argument from how long capacities took to
        evolve.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Bodies Are Not Just for Hookers and Fat People!}

    \includegraphics[width=0.9\textwidth]{bender-without-body.jpg}
  \end{frame}

  \begin{frame}
    \frametitle{Lack of Data}

    \begin{itemize}
      \item Even now, this is a very serious issue!
      \pause
      \item The AI of today, even the most sophisticated, sees the world
        through a pinhole.
      \pause
      \item This frame of the presentation ties into the last.
    \end{itemize}
  \end{frame}

  \begin{frame}
    \frametitle{Objection from Singularitarianism}

    \begin{itemize}
      \item ``Singularitarianism is a movement defined by the belief that a
        technological singularity---the creation of superintelligence---will
        likely happen in the medium future, and that deliberate action ought
        to be taken to ensure that the singularity benefits humans.''
      \pause
      \item The parable of the lily pond.
      \pause
      \item The parable of the state-of-the art Cray supercomputer vs. a
        crappy ThinkCentre.
      \pause
      \item Why I think the Singularity is unlikely however.
    \end{itemize}
  \end{frame}

  % Was the lethal police robot really a robot?
  % Objection from Singularitarianism: Anecdote about number factoring
  % Rodney Brooks' argument about evolution
  % No data, no success

  \section{The Geography of AI Development}
  \section{Impacts on Employment}
  \section{Future of Human-AI Interaction}
  \section{Impact on Philosophy and Religion}
  % Mention kami before 40K
  \section{Impact on Ethics}
  \section{AI and Statistical Literacy}
  \section{Recommended Media}
  % Quest for Artificial Intelligence
  
  \begin{comment}
Subject: Communicating AI Effectively

Possible topics (will likely have to pare them down due to time limitations):

* Taxonomy of AI and its uses:

** Different methods, both at the level of categories (e.g. supervised
learning), and concrete methods (e.g. k-NN). These are most popular these days
but can also be contrasted with traditional methods such as tree search that
dominate texts like Artificial Intelligence: A Modern Approach.

** Applications of these methods (NLP, image classification, computer
opponents, recommender systems, etc.).

* What's realistic and what's not realistic. I'd probably only call anything
involving something like hypercomputation impossible but a lot of things are
just really most likely not going to happen. This would include anything like
Skynet. (This has long been difficult even for luminaries in the field. For
example when Logic Theorist was created and came up with better proofs for
theorems in Principia Mathematica (the Russell and Whitehead one, not the
Newton one) than the human authors, this led to enthusiastic thinking that
human-level or better artificial intelligence would appear in a few decades.
This did not happen.) Of course for all but one of you I am now beating a dead
horse but the most likely path to the superhuman is an enhanced human rather
than something built from scratch. This may or may not include integration of
artificially intelligent components with the human body.

Additional notes:

* Harari has emphasized integration of sensors with human body but only as
eyes and ears of AI!
* Lee is also insightful but has drawn a sharp divide between what is human
and what is technological—the lines will blur!

* Assessment of strengths in development and use of AI technology by country /
region. Primarily this means comparing the USA and PRC. Japan (as well as the
broader Jakota triangle of Japan, ROK and Taiwan), Western Europe, India and
Israel also merit some attention.

* How structural unemployment is likely to play out. ("You will almost
certainly be far safer as a roofer than as a legal researcher...")

* Impact of human-AI interaction in the future. (Japan being a very good
example.)

* Impact of AI and other GRIN (genetic, robotic, information technology,
nanotechnology) technologies on religions and other belief systems. "Dataism"
as put forward by Yuval Noah Harari leading example. Possible compatibility of
traditional religious thinking with mechanistic ideas of the mind typical in
cognitive science assessed as well: Descartes and Hobbes had mechanistic ideas
of the mind and at least one of them (Descartes) was almost definitely a
sincere believer. (Hobbes may have been a closeted atheist.) Japanese notion
of kami where entities not usually considered to have a spirit by other people
also highly relevant—priority of making robots human-like in Japan not likely
coincidental. Fictional examples like the Machine Cult from the card game Star
Realms and Meklar species from the Master of Orion games also worth mentioning
because they reflect on what we think might happen someday.

* Recommended reading / people: both of Yuval Noah Harari's future-oriented
books and The Master Algorithm by Pedro Domingos stand out in my mind
particularly. I also have a high opinion of The Deep Learning Revolution by
Terrence J. Sejnowski. Kinda ambivalent about Weapons of Math Destruction by
Cathy O'Neill because I think the ideological slant ends up in points that
aren't very good in some places but it's still worth a read. Heard good things
about Kai-Fu Lee's book but haven't read it yet. I've read and heard a lot
from Rodney Brooks and generally he's very on point as is another roboticist,
Rolf Pfeifer. Also look into the ideas of J.C.R. Licklider, who was ahead of
the curve by decades—Man-Computer Symbiosis was published in 1960.

* Statistical literacy—most of what we now call "AI" is heavily dependent on
statistics. More technical but includes things like knowing that a p-value is
a conditional probability, statistical vs. practical significance, why one
shouldn't do a whole bunch of significance tests without correcting for
multiple comparisons, frequentism vs. Bayesianism etc.

* Machine ethics: who will get run over by an autonomous car if there is no
other option? Goes into how Isaac Asimov's famous Laws of Robotics don't
really work. (He knew this and often exploited it in his writing.) Military is
of course another issue. There's an interesting book about this, Governing
Lethal Behavior in Autonomous Robots by Ronald C. Arkin. The methods proposed
are kind of high-level and hand-wavey but he does a lot to integrate
traditional Western ethics of war with current prospects of warfighting
machines. Other legal issues. What the technology should be used for.
  \end{comment}
\end{document}
